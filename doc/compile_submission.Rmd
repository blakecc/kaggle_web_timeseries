---
title: "Reviewing data"
output: html_notebook
---

# Load libraries
```{r}
library(tidyverse)
library(stringr)
library(lubridate)
library(data.table)

```

# Load data

```{r}

rm(list = ls())

train_1 <- fread("data/train_2.csv", header = T, encoding = "UTF-8")
key_1 <- fread("data/key_2.csv", header = T, encoding = "UTF-8")
sample_submission_1 <- fread("data/sample_submission_2.csv", header = T, encoding = "UTF-8")

# head(sample_submission_1, 10)
# head(key_1, 10)
# head(train_1[,551], 10)

```

# Split key

```{r}
key_sep <- key_1 %>%
  separate(Page, c("Page", "Date"), sep = -11) %>%
  mutate(Page = str_sub(Page, 1, -2))
```
```{r, eval=F}

# Test page occurence in Train data

sum(!train_1$Page %in% key_sep$Page) / length(train_1$Page) # only 54% match
nomatch_test <- train_1$Page[!train_1$Page %in% key_sep$Page][1]

key_zh_sample <- key_sep %>%
  # filter(Page == nomatch_test) %>%
  filter(str_detect(Page, "zh.wikipedia.org_all-access_spider")) %>%
  sample_n(100)

train_zh_sample <- train_1 %>%
  filter(str_detect(Page, "zh.wikipedia.org_all-access_spider")) %>%
  sample_n(100)

print(key_zh_sample[1,]$Page)
print(train_zh_sample[1,]$Page)

#   mutate(Other = str_sub(Other, 2)) %>%
#   mutate(Lang = str_sub(Lang, 1, -2)) %>%
#   separate(Other, c("device", "agent", "date"), sep = "_") %>%
#   mutate(Topic = str_sub(Topic, 1, -2)) %>%
#   mutate(date = as.POSIXct(date))
# 
# key_1[2220:2230,]
# key_1[2020:2030,]

```

# Construct sample submission
## Basic 10 day MA

```{r}

# average of last 10 days for train data
# train_1$MA_10 <- apply(train_1[,540:551], 1, mean, na.rm = T) # last ten days MA
train_1$MA_60 <- apply(train_1[,(ncol(train_1) - 60):ncol(train_1)], 1, mean, na.rm = T) # last 60 days MA
train_1$Med_60 <- apply(train_1[,(ncol(train_1) - 60):ncol(train_1)], 1, median, na.rm = T) # last 60 days median
# train_1$MA_10 <- train_1[,551] # last value

# train_1 %>%
#   select(Page, MA_10) %>%
#   filter(Page == "\"\"Keep_me_logged_in\"\"_extended_to_one_year_www.mediawiki.org_all-access_all-agents")

train_2 <- train_1 %>%
  select(Page, Med_60)

# head(train_2)

test_submission <- left_join(key_sep, train_2, by = "Page")
# head(test_submission)
# head(sample_submission_1)

submission1 <- test_submission %>%
  mutate(MA_10 = ceiling(Med_60)) %>%
  rename(Visits = Med_60) %>%
  select(Id, Visits) %>%
  mutate(Visits = ifelse(is.na(Visits), 0, Visits)) %>%
  mutate(Visits = as.integer(Visits))

write_csv(submission1, "output/170910_60med_1_key.csv")


# head(submission1)
# head(sample_submission_1)

# head(test_submission, 1000)
# train_1[,c(1,551)]
# 
# train_1[train_1$Page == ".ca_ru.wikipedia.org_desktop_all-agents"][,c(1,540:552)]
```

```{r, eval=F}
submission1 %>%
  filter(Visits <= 50) %>%
  ggplot(aes(Visits)) +
    geom_histogram(bins = 50)

# mean(c(1,2,3,4,NA), na.rm = T)

```

# Using predict_1

```{r}
rm(list = ls())

train_1 <- fread("data/train_1.csv", header = T, encoding = "UTF-8")

source("R/helpers.R")

page_data_1 <- train_1 %>% slice(1)

submission_test <- predict_1(page_data_1)

for (i in 2:nrow(train_1)){
  print(i)
  page_data_1 <- train_1 %>% slice(i)
  predict_output <- suppressWarnings(predict_1(page_data_1))
  submission_test <- rbind(submission_test, predict_output)
}

```